{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"drugsComTrain_raw.csv\", parse_dates=[\"date\"])\n",
    "df_test = pd.read_csv(\"drugsComTest_raw.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(axis=0)\n",
    "df_test = df_test.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train,df_test]).reset_index()\n",
    "del df_all['index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list = set(df_all.index)\n",
    "span_list = []\n",
    "for i,j in enumerate(df_all['condition']):\n",
    "    if '</span>' in j:\n",
    "        span_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_idx = all_list.difference(set(span_list))\n",
    "df_all = df_all.iloc[list(new_idx)].reset_index()\n",
    "del df_all['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condition = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n",
    "df_condition = pd.DataFrame(df_condition).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condition_1 = df_condition[df_condition['drugName']==1].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list = set(df_all.index)\n",
    "condition_list = []\n",
    "for i,j in enumerate(df_all['condition']):\n",
    "    for c in list(df_condition_1['condition']):\n",
    "        if j == c:\n",
    "            condition_list.append(i)\n",
    "            \n",
    "new_idx = all_list.difference(set(condition_list))\n",
    "df_all = df_all.iloc[list(new_idx)].reset_index()\n",
    "del df_all['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_stop = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\n",
    "for i in not_stop:\n",
    "    stops.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in d:\\programs\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\maou\\appdata\\roaming\\python\\python37\\site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: numpy in d:\\programs\\lib\\site-packages (from lightgbm) (1.16.5)\n",
      "Requirement already satisfied: scikit-learn in d:\\programs\\lib\\site-packages (from lightgbm) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\programs\\lib\\site-packages (from scikit-learn->lightgbm) (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # 1. Delete HTML \n",
    "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
    "    # 2. Make a space\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "    # 3. lower letters\n",
    "    words = letters_only.lower().split()\n",
    "    # 5. Stopwords \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # 6. Stemming\n",
    "    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n",
    "    # 7. space join words\n",
    "    return( ' '.join(stemming_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%time df_all['review_clean'] = df_all['review'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['sentiment'] = df_all[\"rating\"].apply(lambda x: 1 if x > 5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_all, test_size=0.33, random_state=42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             min_df = 2, \n",
    "                             ngram_range=(4, 4),\n",
    "                             max_features = 20000\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%time train_data_features = pipeline.fit_transform(df_train['review_clean'])\n",
    "%time test_data_features = pipeline.fit_transform(df_test['review_clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code in keras 김태영'blog\n",
    "# 0. Package\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. Dataset\n",
    "y_train = df_train['sentiment']\n",
    "y_test = df_test['sentiment']\n",
    "solution = y_test.copy()\n",
    "\n",
    "# 2. Model Structure\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(200, input_shape=(20000,)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(300))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. Model compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               4000200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,092,701\n",
      "Trainable params: 4,091,701\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "142075/142075 [==============================] - 145s 1ms/step - loss: 0.5797 - accuracy: 0.7149\n",
      "Epoch 2/10\n",
      "142075/142075 [==============================] - 146s 1ms/step - loss: 0.4974 - accuracy: 0.7597\n",
      "Epoch 3/10\n",
      "142075/142075 [==============================] - 146s 1ms/step - loss: 0.4622 - accuracy: 0.7752\n",
      "Epoch 4/10\n",
      "142075/142075 [==============================] - 146s 1ms/step - loss: 0.4418 - accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "142075/142075 [==============================] - 146s 1ms/step - loss: 0.4280 - accuracy: 0.7902\n",
      "Epoch 6/10\n",
      "142075/142075 [==============================] - 145s 1ms/step - loss: 0.4182 - accuracy: 0.7948\n",
      "Epoch 7/10\n",
      "142075/142075 [==============================] - 145s 1ms/step - loss: 0.4115 - accuracy: 0.7978\n",
      "Epoch 8/10\n",
      "142075/142075 [==============================] - 149s 1ms/step - loss: 0.4057 - accuracy: 0.7997\n",
      "Epoch 9/10\n",
      "142075/142075 [==============================] - 148s 1ms/step - loss: 0.4013 - accuracy: 0.8020\n",
      "Epoch 10/10\n",
      "142075/142075 [==============================] - 153s 1ms/step - loss: 0.3981 - accuracy: 0.8033\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_data_features, y_train, epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69978/69978 [==============================] - 20s 282us/step\n",
      "loss_and_metrics : [1.0560347685687435, 0.648789644241333]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(test_data_features, y_test, batch_size=32)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_preds_deep = model.predict(test_data_features,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.570406\tvalid_1's binary_logloss: 0.572234\n",
      "[200]\ttraining's binary_logloss: 0.570206\tvalid_1's binary_logloss: 0.572221\n",
      "Early stopping, best iteration is:\n",
      "[168]\ttraining's binary_logloss: 0.570241\tvalid_1's binary_logloss: 0.572209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "target = df_train['sentiment']\n",
    "feats = ['usefulCount']\n",
    "\n",
    "sub_preds = np.zeros(df_test.shape[0])\n",
    "\n",
    "trn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \n",
    "feature_importance_df = pd.DataFrame() \n",
    "    \n",
    "clf = LGBMClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=30,\n",
    "        #colsample_bytree=.9,\n",
    "        subsample=.9,\n",
    "        max_depth=7,\n",
    "        reg_alpha=.1,\n",
    "        reg_lambda=.1,\n",
    "        min_split_gain=.01,\n",
    "        min_child_weight=2,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "        )\n",
    "        \n",
    "clf.fit(trn_x, trn_y, \n",
    "        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "        verbose=100, early_stopping_rounds=100  #30\n",
    "    )\n",
    "\n",
    "sub_preds = clf.predict(df_test[feats])\n",
    "        \n",
    "fold_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"feature\"] = feats\n",
    "fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 21009],\n",
       "       [    0, 48969]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = df_test['sentiment']\n",
    "confusion_matrix(y_pred=sub_preds, y_true=solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = df_train.shape[0]\n",
    "df_all = pd.concat([df_train,df_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "df_all['day'] = df_all['date'].dt.day\n",
    "df_all['year'] = df_all['date'].dt.year\n",
    "df_all['month'] = df_all['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in d:\\programs\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\programs\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in d:\\programs\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 212053/212053 [01:18<00:00, 2688.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>Predict_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>130185</td>\n",
       "      <td>66913</td>\n",
       "      <td>Seroquel</td>\n",
       "      <td>Generalized Anxiety Disorde</td>\n",
       "      <td>\"After trying nearly every SSRI on the market ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2010-11-26</td>\n",
       "      <td>102</td>\n",
       "      <td>tri near everi ssri market place pristiq impro...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>0.023958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155501</td>\n",
       "      <td>222222</td>\n",
       "      <td>Fluconazole</td>\n",
       "      <td>Onychomycosis, Toenail</td>\n",
       "      <td>\"This takes 6+ months, but did clear up a deca...</td>\n",
       "      <td>9</td>\n",
       "      <td>2008-05-03</td>\n",
       "      <td>39</td>\n",
       "      <td>take month clear decad long infect</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75325</td>\n",
       "      <td>75807</td>\n",
       "      <td>Temazepam</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>\"Worked for awhile pretty well but then went b...</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>15</td>\n",
       "      <td>work awhil pretti well went back mayb get hour...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19174</td>\n",
       "      <td>229747</td>\n",
       "      <td>Ketamine</td>\n",
       "      <td>Pain</td>\n",
       "      <td>\"I was given this after surgery for pain as I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>19</td>\n",
       "      <td>given surgeri pain morphin tri drug sort omg w...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209735</td>\n",
       "      <td>102495</td>\n",
       "      <td>Aripiprazole</td>\n",
       "      <td>Major Depressive Disorde</td>\n",
       "      <td>\"Abilify served me well over a two month perio...</td>\n",
       "      <td>9</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>5</td>\n",
       "      <td>abilifi serv well two month period antidepress...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0.038889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueID      drugName                    condition  \\\n",
       "130185     66913      Seroquel  Generalized Anxiety Disorde   \n",
       "155501    222222   Fluconazole       Onychomycosis, Toenail   \n",
       "75325      75807     Temazepam                     Insomnia   \n",
       "19174     229747      Ketamine                         Pain   \n",
       "209735    102495  Aripiprazole     Major Depressive Disorde   \n",
       "\n",
       "                                                   review  rating       date  \\\n",
       "130185  \"After trying nearly every SSRI on the market ...       9 2010-11-26   \n",
       "155501  \"This takes 6+ months, but did clear up a deca...       9 2008-05-03   \n",
       "75325   \"Worked for awhile pretty well but then went b...       2 2016-04-21   \n",
       "19174   \"I was given this after surgery for pain as I ...       1 2016-04-16   \n",
       "209735  \"Abilify served me well over a two month perio...       9 2016-12-16   \n",
       "\n",
       "        usefulCount                                       review_clean  \\\n",
       "130185          102  tri near everi ssri market place pristiq impro...   \n",
       "155501           39                 take month clear decad long infect   \n",
       "75325            15  work awhil pretti well went back mayb get hour...   \n",
       "19174            19  given surgeri pain morphin tri drug sort omg w...   \n",
       "209735            5  abilifi serv well two month period antidepress...   \n",
       "\n",
       "        sentiment  day  year  month  Predict_Sentiment  \n",
       "130185          1   26  2010     11           0.023958  \n",
       "155501          1    3  2008      5           0.025000  \n",
       "75325           0   21  2016      4          -0.125000  \n",
       "19174           0   16  2016      4           0.166667  \n",
       "209735          1   16  2016     12           0.038889  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "reviews = df_all['review_clean']\n",
    "\n",
    "Predict_Sentiment = []\n",
    "for review in tqdm(reviews):\n",
    "    blob = TextBlob(review)\n",
    "    Predict_Sentiment += [blob.sentiment.polarity]\n",
    "df_all[\"Predict_Sentiment\"] = Predict_Sentiment\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.25709864],\n",
       "       [0.25709864, 1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.23518272],\n",
       "       [0.23518272, 1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 212053/212053 [02:37<00:00, 1343.57it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = df_all['review']\n",
    "\n",
    "Predict_Sentiment = []\n",
    "for review in tqdm(reviews):\n",
    "    blob = TextBlob(review)\n",
    "    Predict_Sentiment += [blob.sentiment.polarity]\n",
    "df_all[\"Predict_Sentiment2\"] = Predict_Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.31714515],\n",
       "       [0.31714515, 1.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"rating\"])\n",
    "np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_all['count_sent']=df_all[\"review\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "#Word count in each comment:(단어갯수)\n",
    "df_all['count_word']=df_all[\"review_clean\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Unique word count(unique한 단어 갯수)\n",
    "df_all['count_unique_word']=df_all[\"review_clean\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "#Letter count(리뷰길이)\n",
    "df_all['count_letters']=df_all[\"review_clean\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "#punctuation count(특수문자)\n",
    "df_all[\"count_punctuations\"] = df_all[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "#upper case words count(전부다 대문자인 단어 갯수)\n",
    "df_all[\"count_words_upper\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "#title case words count(첫글자가 대문자인 단어 갯수)\n",
    "df_all[\"count_words_title\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#Number of stopwords(불용어 갯수)\n",
    "df_all[\"count_stopwords\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n",
    "\n",
    "#Average length of the words(평균단어길이)\n",
    "df_all[\"mean_word_len\"] = df_all[\"review_clean\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['season'] = df_all[\"month\"].apply(lambda x: 1 if ((x>2) & (x<6)) else(2 if (x>5) & (x<9) else (3 if (x>8) & (x<12) else 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 21009],\n",
       "       [    0, 48969]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=sub_preds, y_true=solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table = pd.read_csv(\"inquirerbasic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Positiv word list   \n",
    "temp_Positiv = []\n",
    "Positiv_word_list = []\n",
    "for i in range(0,len(word_table.Positiv)):\n",
    "    if word_table.iloc[i,2] == \"Positiv\":\n",
    "        temp = word_table.iloc[i,0].lower()\n",
    "        temp1 = re.sub('\\d+', '', temp)\n",
    "        temp2 = re.sub('#', '', temp1) \n",
    "        temp_Positiv.append(temp2)\n",
    "\n",
    "Positiv_word_list = list(set(temp_Positiv))\n",
    "len(temp_Positiv)\n",
    "len(Positiv_word_list)  #del temp_Positiv\n",
    "\n",
    "#Negativ word list          \n",
    "temp_Negativ = []\n",
    "Negativ_word_list = []\n",
    "for i in range(0,len(word_table.Negativ)):\n",
    "    if word_table.iloc[i,3] == \"Negativ\":\n",
    "        temp = word_table.iloc[i,0].lower()\n",
    "        temp1 = re.sub('\\d+', '', temp)\n",
    "        temp2 = re.sub('#', '', temp1) \n",
    "        temp_Negativ.append(temp2)\n",
    "\n",
    "Negativ_word_list = list(set(temp_Negativ))\n",
    "len(temp_Negativ)\n",
    "len(Negativ_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary = Positiv_word_list)\n",
    "content = df_test['review_clean']\n",
    "X = vectorizer.fit_transform(content)\n",
    "f = X.toarray()\n",
    "f = pd.DataFrame(f)\n",
    "f.columns=Positiv_word_list\n",
    "df_test[\"num_Positiv_word\"] = f.sum(axis=1)\n",
    "\n",
    "vectorizer2 = CountVectorizer(vocabulary = Negativ_word_list)\n",
    "content = df_test['review_clean']\n",
    "X2 = vectorizer2.fit_transform(content)\n",
    "f2 = X2.toarray()\n",
    "f2 = pd.DataFrame(f2)\n",
    "f2.columns=Negativ_word_list\n",
    "df_test[\"num_Negativ_word\"] = f2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_Positiv_word</th>\n",
       "      <th>num_Negativ_word</th>\n",
       "      <th>Positiv_ratio</th>\n",
       "      <th>sentiment_by_dic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>147354</td>\n",
       "      <td>126702</td>\n",
       "      <td>Brimonidine</td>\n",
       "      <td>Rosacea</td>\n",
       "      <td>\"Never , never , never use this cream !!!!!! I...</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>17</td>\n",
       "      <td>never never never use cream help way angri dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195249</td>\n",
       "      <td>84765</td>\n",
       "      <td>Ethinyl estradiol / norgestimate</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I was switched to this after about 9 months o...</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>3</td>\n",
       "      <td>switch month mononessa ask switch happen anywa...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86265</td>\n",
       "      <td>71559</td>\n",
       "      <td>Vraylar</td>\n",
       "      <td>Bipolar Disorde</td>\n",
       "      <td>\"I began at 1.5 then titrated to 3mg dosage. A...</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-08-17</td>\n",
       "      <td>17</td>\n",
       "      <td>began titrat mg dosag first love medicin compl...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44172</td>\n",
       "      <td>132320</td>\n",
       "      <td>Ativan</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>\"Ativan 0.5mg calms me down considerably withi...</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>26</td>\n",
       "      <td>ativan mg calm consider within minut side effe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90569</td>\n",
       "      <td>177278</td>\n",
       "      <td>Duloxetine</td>\n",
       "      <td>Generalized Anxiety Disorde</td>\n",
       "      <td>\"My thoughts on Cymbalta: Cymbalta has left me...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>29</td>\n",
       "      <td>thought cymbalta cymbalta left fog no feel non...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueID                          drugName  \\\n",
       "147354    126702                       Brimonidine   \n",
       "195249     84765  Ethinyl estradiol / norgestimate   \n",
       "86265      71559                           Vraylar   \n",
       "44172     132320                            Ativan   \n",
       "90569     177278                        Duloxetine   \n",
       "\n",
       "                          condition  \\\n",
       "147354                      Rosacea   \n",
       "195249                Birth Control   \n",
       "86265               Bipolar Disorde   \n",
       "44172                       Anxiety   \n",
       "90569   Generalized Anxiety Disorde   \n",
       "\n",
       "                                                   review  rating       date  \\\n",
       "147354  \"Never , never , never use this cream !!!!!! I...       1 2015-08-31   \n",
       "195249  \"I was switched to this after about 9 months o...      10 2016-05-01   \n",
       "86265   \"I began at 1.5 then titrated to 3mg dosage. A...       3 2016-08-17   \n",
       "44172   \"Ativan 0.5mg calms me down considerably withi...       5 2012-01-12   \n",
       "90569   \"My thoughts on Cymbalta: Cymbalta has left me...       1 2017-05-03   \n",
       "\n",
       "        usefulCount                                       review_clean  \\\n",
       "147354           17  never never never use cream help way angri dis...   \n",
       "195249            3  switch month mononessa ask switch happen anywa...   \n",
       "86265            17  began titrat mg dosag first love medicin compl...   \n",
       "44172            26  ativan mg calm consider within minut side effe...   \n",
       "90569            29  thought cymbalta cymbalta left fog no feel non...   \n",
       "\n",
       "        sentiment  num_Positiv_word  num_Negativ_word  Positiv_ratio  \\\n",
       "147354          0               NaN               NaN            NaN   \n",
       "195249          1               NaN               NaN            NaN   \n",
       "86265           0               NaN               NaN            NaN   \n",
       "44172           0               0.0               4.0            0.0   \n",
       "90569           0               NaN               NaN            NaN   \n",
       "\n",
       "        sentiment_by_dic  \n",
       "147354               0.5  \n",
       "195249               0.5  \n",
       "86265                0.5  \n",
       "44172                0.0  \n",
       "90569                0.5  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##3. decide sentiment\n",
    "df_test[\"Positiv_ratio\"] = df_test[\"num_Positiv_word\"]/(df_test[\"num_Positiv_word\"]+df_test[\"num_Negativ_word\"])\n",
    "df_test[\"sentiment_by_dic\"] = df_test[\"Positiv_ratio\"].apply(lambda x: 1 if (x>=0.5) else (0 if (x<0.5) else 0.5))\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userful_count(data):\n",
    "    grouped = data.groupby(['condition']).size().reset_index(name='user_size')\n",
    "    data = pd.merge(data,grouped,on='condition',how='left')\n",
    "    return data\n",
    "#___________________________________________________________\n",
    "df_test =  userful_count(df_test) \n",
    "df_test['usefulCount'] = df_test['usefulCount']/df_test['user_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['deep_pred'] = sub_preds_deep\n",
    "df_test['machine_pred'] = sub_preds\n",
    "\n",
    "df_test['total_pred'] = (df_test['deep_pred'] + df_test['machine_pred'] + df_test['sentiment_by_dic'])*df_test['usefulCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <th>drugName</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">ADHD</td>\n",
       "      <td>Adderall</td>\n",
       "      <td>0.071786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Adderall XR</td>\n",
       "      <td>0.043242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Adzenys XR-ODT</td>\n",
       "      <td>0.012092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amantadine</td>\n",
       "      <td>0.011008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amphetamine</td>\n",
       "      <td>0.015865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">moterol)</td>\n",
       "      <td>Arformoterol</td>\n",
       "      <td>2.044659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Budesonide / formoterol</td>\n",
       "      <td>2.342874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>von Willebrand's Disease</td>\n",
       "      <td>Stimate</td>\n",
       "      <td>7.013450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">zen Shoulde</td>\n",
       "      <td>Nabumetone</td>\n",
       "      <td>27.773980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Naproxen</td>\n",
       "      <td>1.448300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5773 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 total_pred\n",
       "                                                       mean\n",
       "condition                drugName                          \n",
       "ADHD                     Adderall                  0.071786\n",
       "                         Adderall XR               0.043242\n",
       "                         Adzenys XR-ODT            0.012092\n",
       "                         Amantadine                0.011008\n",
       "                         Amphetamine               0.015865\n",
       "...                                                     ...\n",
       "moterol)                 Arformoterol              2.044659\n",
       "                         Budesonide / formoterol   2.342874\n",
       "von Willebrand's Disease Stimate                   7.013450\n",
       "zen Shoulde              Nabumetone               27.773980\n",
       "                         Naproxen                  1.448300\n",
       "\n",
       "[5773 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.groupby(['condition','drugName']).agg({'total_pred' : ['mean']})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
